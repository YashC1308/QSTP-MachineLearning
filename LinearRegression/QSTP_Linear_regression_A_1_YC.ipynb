{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QSTP_Linear-regression_A-1_YC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyUs5sl+JcXi9WZR43Cg6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashC1308/QSTP-MachineLearning/blob/main/QSTP_Linear_regression_A_1_YC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "IVw8lQhQ5Tj_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import shape\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.read_csv('QSTP_LinReg.csv')\n",
        "\n",
        "\n",
        "features = ['AT','V','AP','RH']\n",
        "X = df.loc[:, features]\n",
        "y = df.loc[:, ['PE']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .80)\n",
        "\n",
        "norm = MinMaxScaler().fit(X_train)\n",
        "X_train_norm = norm.transform(X_train)\n",
        "X_test_norm = norm.transform(X_test)\n",
        "\n",
        "norm = MinMaxScaler().fit(y_train)\n",
        "y_train_norm = norm.transform(y_train)\n",
        "y_test_norm = norm.transform(y_test)\n",
        "\n",
        "n_samples = y_train_norm.size  # No. of samples taken\n",
        "n_features = 4  # No. of features / No. of values in X\n",
        "n_targets = 1  # No. of target variables / No. of values in y\n"
      ],
      "metadata": {
        "id": "dVKhlDRu0bMH"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function\n",
        "def loss_fn(y_hat, y):\n",
        "    return 1/n_samples * np.sum((y_hat - y)**2)"
      ],
      "metadata": {
        "id": "dxrcW1m_wts_"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gradient descent implementation\n",
        "\n",
        "W = np.random.normal(size=(n_targets, n_features))\n",
        "b = np.random.normal(size=(n_targets, 1))\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "learning_rate =   0.01 \n",
        "num_epochs = 100  \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss = 0\n",
        "    for i in range(n_samples):\n",
        "        X_i, y_i = X_train_norm[i].reshape(-1,1), y_train_norm[i].reshape(-1,1)\n",
        "        y_hat = np.dot(W, X_i) + b\n",
        "        \n",
        "        dL_dy = 2*(y_hat-y_i)\n",
        "        \n",
        "        dy_dW = X_i\n",
        "        dy_db = 1\n",
        "        \n",
        "        dL_dW = np.dot(dL_dy, dy_dW.T)\n",
        "        dL_db = dL_dy * dy_db\n",
        "        \n",
        "        W -= learning_rate * dL_dW\n",
        "        b -= learning_rate * dL_db\n",
        "\n",
        "        loss += loss_fn(y_hat, y_i)\n",
        "    \n",
        "    loss_history.append(loss)\n",
        "    print(\"Epoch: {}, Loss: {}\".format(epoch+1, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biSV2oaByyhT",
        "outputId": "0ccba469-7c9f-4d30-88ef-5cfa025a582a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 0.03192479064326155\n",
            "Epoch: 2, Loss: 0.004459955497903041\n",
            "Epoch: 3, Loss: 0.003892141638861122\n",
            "Epoch: 4, Loss: 0.0038171366316638224\n",
            "Epoch: 5, Loss: 0.0037964323287142062\n",
            "Epoch: 6, Loss: 0.0037896072848122232\n",
            "Epoch: 7, Loss: 0.0037872465620643974\n",
            "Epoch: 8, Loss: 0.0037863932190413175\n",
            "Epoch: 9, Loss: 0.003786066214278786\n",
            "Epoch: 10, Loss: 0.003785931496889216\n",
            "Epoch: 11, Loss: 0.0037858714726361526\n",
            "Epoch: 12, Loss: 0.003785842703628367\n",
            "Epoch: 13, Loss: 0.0037858280770322775\n",
            "Epoch: 14, Loss: 0.0037858203183179793\n",
            "Epoch: 15, Loss: 0.0037858160857968002\n",
            "Epoch: 16, Loss: 0.003785813736250421\n",
            "Epoch: 17, Loss: 0.003785812418245071\n",
            "Epoch: 18, Loss: 0.003785811674334208\n",
            "Epoch: 19, Loss: 0.0037858112529558898\n",
            "Epoch: 20, Loss: 0.0037858110137822362\n",
            "Epoch: 21, Loss: 0.003785810877868289\n",
            "Epoch: 22, Loss: 0.0037858108005814086\n",
            "Epoch: 23, Loss: 0.003785810756615801\n",
            "Epoch: 24, Loss: 0.003785810731599928\n",
            "Epoch: 25, Loss: 0.003785810717364483\n",
            "Epoch: 26, Loss: 0.0037858107092631267\n",
            "Epoch: 27, Loss: 0.0037858107046524776\n",
            "Epoch: 28, Loss: 0.003785810702028403\n",
            "Epoch: 29, Loss: 0.0037858107005349353\n",
            "Epoch: 30, Loss: 0.0037858106996849546\n",
            "Epoch: 31, Loss: 0.003785810699201156\n",
            "Epoch: 32, Loss: 0.0037858106989258106\n",
            "Epoch: 33, Loss: 0.003785810698769123\n",
            "Epoch: 34, Loss: 0.0037858106986799174\n",
            "Epoch: 35, Loss: 0.003785810698629153\n",
            "Epoch: 36, Loss: 0.0037858106986002667\n",
            "Epoch: 37, Loss: 0.003785810698583835\n",
            "Epoch: 38, Loss: 0.0037858106985744627\n",
            "Epoch: 39, Loss: 0.00378581069856913\n",
            "Epoch: 40, Loss: 0.0037858106985661152\n",
            "Epoch: 41, Loss: 0.0037858106985643987\n",
            "Epoch: 42, Loss: 0.003785810698563383\n",
            "Epoch: 43, Loss: 0.003785810698562842\n",
            "Epoch: 44, Loss: 0.003785810698562516\n",
            "Epoch: 45, Loss: 0.0037858106985623374\n",
            "Epoch: 46, Loss: 0.003785810698562227\n",
            "Epoch: 47, Loss: 0.00378581069856217\n",
            "Epoch: 48, Loss: 0.0037858106985621406\n",
            "Epoch: 49, Loss: 0.003785810698562131\n",
            "Epoch: 50, Loss: 0.003785810698562127\n",
            "Epoch: 51, Loss: 0.0037858106985621085\n",
            "Epoch: 52, Loss: 0.0037858106985621085\n",
            "Epoch: 53, Loss: 0.003785810698562095\n",
            "Epoch: 54, Loss: 0.0037858106985621\n",
            "Epoch: 55, Loss: 0.003785810698562093\n",
            "Epoch: 56, Loss: 0.003785810698562095\n",
            "Epoch: 57, Loss: 0.003785810698562095\n",
            "Epoch: 58, Loss: 0.0037858106985620946\n",
            "Epoch: 59, Loss: 0.0037858106985620946\n",
            "Epoch: 60, Loss: 0.003785810698562094\n",
            "Epoch: 61, Loss: 0.0037858106985620946\n",
            "Epoch: 62, Loss: 0.003785810698562094\n",
            "Epoch: 63, Loss: 0.0037858106985620946\n",
            "Epoch: 64, Loss: 0.0037858106985620946\n",
            "Epoch: 65, Loss: 0.003785810698562094\n",
            "Epoch: 66, Loss: 0.003785810698562094\n",
            "Epoch: 67, Loss: 0.0037858106985620946\n",
            "Epoch: 68, Loss: 0.003785810698562094\n",
            "Epoch: 69, Loss: 0.0037858106985620946\n",
            "Epoch: 70, Loss: 0.0037858106985620946\n",
            "Epoch: 71, Loss: 0.003785810698562094\n",
            "Epoch: 72, Loss: 0.003785810698562094\n",
            "Epoch: 73, Loss: 0.0037858106985620946\n",
            "Epoch: 74, Loss: 0.003785810698562094\n",
            "Epoch: 75, Loss: 0.0037858106985620946\n",
            "Epoch: 76, Loss: 0.0037858106985620946\n",
            "Epoch: 77, Loss: 0.003785810698562094\n",
            "Epoch: 78, Loss: 0.003785810698562094\n",
            "Epoch: 79, Loss: 0.0037858106985620946\n",
            "Epoch: 80, Loss: 0.003785810698562094\n",
            "Epoch: 81, Loss: 0.0037858106985620946\n",
            "Epoch: 82, Loss: 0.0037858106985620946\n",
            "Epoch: 83, Loss: 0.003785810698562094\n",
            "Epoch: 84, Loss: 0.003785810698562094\n",
            "Epoch: 85, Loss: 0.0037858106985620946\n",
            "Epoch: 86, Loss: 0.003785810698562094\n",
            "Epoch: 87, Loss: 0.0037858106985620946\n",
            "Epoch: 88, Loss: 0.0037858106985620946\n",
            "Epoch: 89, Loss: 0.003785810698562094\n",
            "Epoch: 90, Loss: 0.003785810698562094\n",
            "Epoch: 91, Loss: 0.0037858106985620946\n",
            "Epoch: 92, Loss: 0.003785810698562094\n",
            "Epoch: 93, Loss: 0.0037858106985620946\n",
            "Epoch: 94, Loss: 0.0037858106985620946\n",
            "Epoch: 95, Loss: 0.003785810698562094\n",
            "Epoch: 96, Loss: 0.003785810698562094\n",
            "Epoch: 97, Loss: 0.0037858106985620946\n",
            "Epoch: 98, Loss: 0.003785810698562094\n",
            "Epoch: 99, Loss: 0.0037858106985620946\n",
            "Epoch: 100, Loss: 0.0037858106985620946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot change in history. X-axis is the epochs\n",
        "#how quickly W and b were achieved and loss history\n",
        "plt.plot(loss_history)\n",
        "print(W)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ca7EzdivA8yH",
        "outputId": "af6c5be5-3f3d-42f1-a43b-716b9d1651b7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.91995627 -0.1751863   0.03166132 -0.15920537]]\n",
            "[[1.08597538]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "square_diff =[]\n",
        "mean_diff = []\n",
        "mean = np.mean(y_test_norm)\n",
        "\n",
        "for i in range(y_test_norm.size):\n",
        "  X_i, y_i = X_test_norm[i].reshape(-1,1), y_test_norm[i].reshape(-1,1)\n",
        "  pred_values = np.dot(W, X_i) + b\n",
        "  square_diff.append((y_i - pred_values)**2)\n",
        "  mean_diff.append((y_i - mean)**2)\n",
        "\n",
        "sum1 = 0\n",
        "sum2 = 0\n",
        "\n",
        "for i in range(len(square_diff)):\n",
        "  sum1+=square_diff[i]\n",
        "  sum2+=mean_diff[i]\n",
        "\n",
        "r_square = 1 - (sum1/sum2)\n",
        "\n",
        "print(\"R-squared value\")\n",
        "print(r_square)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMZyi48oBuW2",
        "outputId": "b7a89bc4-34c0-4d71-9a4b-b67fc669b1ed"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared value\n",
            "[[0.93157973]]\n"
          ]
        }
      ]
    }
  ]
}